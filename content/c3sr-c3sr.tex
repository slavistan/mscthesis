\chapter{3-Fold Compressed Sparse Row Matrix Storage Format}

  Evidently, representing a sparse banded matrix using the regular CSR format is suboptimal, as it provides no means of
  capturing the apparent repetitiveness of the structure. While at first glance the diagonal format might seem an
  appealing choice for the types of matrices introduced in the previous section, real-life problems produce matrices
  which are possibly only locally structured, i.e. they contain multiple fully structured sections corresponding to the
  multiple structured grid regions of the overall heterogeneous domain (Figure \ref{fig:refined_structured_grid}), which
  need not be aligned in a way to produce a diagonal structure at all. For such matrices the diagonal storage format is
  highly suboptimal \cite{Bell2011}. Thus a more flexible approach is taken adapting the general purpose CSR format to
  better suit the characteristics of sparse banded matrices.

  This section introduces the data layout and the matrix-vector multiplication scheme of the \keyword{3-fold compressed
  sparse row} matrix storage format (C3SR). Its raison d'être is to optimize arithmetic performance of matrix-vector
  multiplications involving sparse banded matrices by improving data locality through a structurally-aware space-saving
  storage scheme and by implementing an efficient arithmetic scheme capable of profiting from vectorization and
  multi-threading.

  \section{Data Layout and Storage Scheme}

    \Todo{Erläutern, dass 'structure of a matrix' die Positionen der Nonzeros und 'values' die numerischen Werte meint}

    It is crucial to observe that, in the most general case, a regularity inherent to a sparse banded matrix's
    non-zeros' column indices is not shared by the non-zeros' numerical values. While the non-zeros of two or more rows
    may share the same column indices, save for a possible offset, their corresponding values need not to be similar to
    each other at all. To prevent that a lack of common regularity impedes optimizing the storage of one or the other it
    is thus necessary to decouple the representation of a row's non-zeros' column index positions from the
    representation of their numerical values. The C3SR format accounts for this circumstance and maintains separate data
    structures for the non-zeros' column indices and their values. Hence the nomenclature of the C3SR format, as it is
    based on the idea of the CSR format and adds another two potential layers of compression: one for the column indices
    and one for the values.

    Based on the observation that sparse banded matrices contain many rows whose non-zeros are located at the same
    positions except for a possible offset the C3SR format decomposes the column index positions into the column index
    of the row's first non-zero element, referred to as the \keyword{row's peg} hereinafter, and the relative column
    indices of all of the row's non-zeros with respect to the first non-zero's column index, i.e. the relative offsets
    with respect to the peg index. The latter column index offsets relative to the peg index shall be called the
    \keyword{row's column index pattern}, or simply the \keyword{row's pattern}. Naturally, only unique patterns are stored,
    drastically shrinking the storage requirements of such matrices whose majority of rows exhibit the same pattern.

    Thus, in order to represent the column indices of a matrix row's non-zeros the C3SR format utilizes three arrays:

    \begin{description}[align = left, labelwidth = 4cm]
      \item [JP - \emph{Peg column indices}] \hfill \\
        Column index of each row's first non-zero. One element per row in matrix.
      \item [J - \emph{Column index patterns}] \hfill \\
        Column index position offsets of the rows' non-zeros relative to the peg
        column index for each row. Only unique patterns are stored.
      \item [JS - \emph{Patterns' index-pointers}] \hfill \\
        Index-pointer to each row's first element within J. One element per row in matrix.
    \end{description}

    An example matrix and its corresponding structural information are given in Figure \ref{fig:c3sr_example_structure}.

    \begin{figure}[ht]
      \centering
      \captionsetup{width=.9\textwidth}
      \begin{minipage}{0.4\textwidth}
        \centering
        $$
        \begin{pmatrix}
          0 & \bullet & 0 & \bullet & \bullet & 0 \\
          0 & \bullet & 0 & 0 & \bullet & 0 \\
          0 & 0 & \bullet & 0 & \bullet & \bullet \\
        \end{pmatrix}
        $$
      \end{minipage}
      \begin{minipage}{0.4\textwidth}
        \centering
        $$
        \begin{matrix}
          JP & : & 1 & 1 & 2 &   &   \\
           J & : & 0 & 2 & 3 & 0 & 3 \\
          JS & : & 0 & 3 & 0 &   &   \\
        \end{matrix}
        $$
      \end{minipage}
      \toccaption{Matrix with corresponding C3SR representation (structural information only).}{The non-zeros are
        denoted as black dots. The $0^{\text{th}}$ and $2^{\text{th}}$ row exhibit the same column index pattern
        $\big(0,2,3\big)$ at different peg index positions of $1$ and $2$, respectively. Thus the $0^{\text{th}}$ and
        the $2^{\text{th}}$ element of JS point J[0], which is the first element of this unique pattern. The
        $1^{\text{th}}$ row has a unique pattern which is stored after the previous pattern within J at offset $3$ which
        JS[1] accordingly points.}
      \label{fig:c3sr_example_structure}
    \end{figure}

    The matrix's non-zeros' numerical values are represented using two dense arrays: V, which contains the actual values
    and VS, the index-pointer into V which relates to V in the same way JS relates to J. As with J, row values are
    stored uniquely: Multiple rows with identical non-zeros (with possibly differing columns) lead to a single set of
    entries in V.

    \begin{description}[align = left, labelwidth = 4cm]
      \item [V - \emph{Values}] \hfill \\
        Non-zeros' numerical values. Duplicates across rows with the same pattern are stored only once.
      \item [VS - \emph{Values' index-pointers}] \hfill \\
        Index-pointer to each row's first element within V. One element per row in matrix.
    \end{description}

    An additional sixth array RS stores the number of non-zeros in each row of the matrix. This information is required
    as the index-pointer arrays JS and VS point a row's first element within J and V but, in contrast to the general CSR
    format, does not contain the information about how long the segment pertaining to the row within those arrays is. By
    virtue of the patterns' and values' storage schemes multiple rows' non-zeros' patterns and values may correspond to
    the very same portion of J and V such that the end of that portion cannot be retrieved from the corresponding
    index-pointer arrays JS and VS as could be done for the CSR format by taking the difference of two consecutive
    index-pointers from RP.

    \begin{description}[align = left, labelwidth = 4cm]
      \item [RS - \emph{Row sizes}] \hfill \\
        Number of non-zeros for each row. One element per row in matrix.
    \end{description}

    Another exemplary matrix and its full C3SR format representation are given in Figure \ref{fig:c3sr_example_full}.

    \begin{figure}[ht]
      \centering
      \begin{minipage}{0.4\textwidth}
        \centering
        $$
        \begin{pmatrix}
          1 & 2 & 0 & 3 & 0 & 0 \\
          0 & 1 & 2 & 0 & 3 & 0 \\
          0 & 0 & 4 & 5 & 0 & 0 \\
          0 & 0 & 6 & 7 & 0 & 0 \\
        \end{pmatrix}
        $$
      \end{minipage}
      \begin{minipage}{0.4\textwidth}
        \centering
        $$
        \begin{matrix}
          JP & : & 0 & 1 & 2 & 2 &   &   &   \\
           J & : & 0 & 1 & 3 & 0 & 1 &   &   \\
          JS & : & 0 & 0 & 3 & 3 &   &   &   \\
           V & : & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
          VS & : & 0 & 0 & 3 & 5 &   &   &   \\
          \RS & : & 3 & 3 & 2 & 2 &   &   &   \\
        \end{matrix}
        $$
      \end{minipage}
      \caption[Matrix with corresponding full C3SR representation.]{\textbf{Matrix with corresponding full C3SR representation.} The $0^{\text{th}}$ and $2^{\text{th}}$ rows exhibit the same pattern and the same values, thus the values are stored only once. The last two rows also share a common pattern but with different values.}
      \label{fig:c3sr_example_full}
    \end{figure}

    By design of the storage scheme, sparse banded matrices such as the example shown in Figure
    \ref{fig:laplacian-example} require very few elements within J as a structured grid's inner nodes, whose
    corresponding rows in the sparse banded matrix exhibit the same pattern which is only stored once, comprise the vast
    majority of all nodes. Depending on the underlying physical problem this property also applies to V if all rows of
    an equal pattern share the same set of values. 

    As will be detailed in the following section \ref{subsec:matrix-vector-multiplication-schemes} a matrix-vector
    multiplication involves copiously accessing J and V the C3SR format greatly improves the spatial locality of the
    data fields most pertinent to arithmetic. A best-case example is given in Figure \ref{fig:c3sr-example-best-case}.

    \begin{figure}[ht]
      \centering
      \begin{minipage}{0.5\textwidth}
        \centering
        $$
        \begin{pmatrix}
          1 & 0 & 2 & 0 & 3 & 4 & 0 & 0 & 0 & 0 \\
          0 & 1 & 0 & 2 & 0 & 3 & 4 & 0 & 0 & 0 \\
            &   & \ddots &   & \ddots &   & \ddots & \ddots \\
          0 & 0 & 0 & 1 & 0 & 2 & 0 & 3 & 4 & 0 \\
          0 & 0 & 0 & 0 & 1 & 0 & 2 & 0 & 3 & 4 \\
        \end{pmatrix}
        $$
      \end{minipage}
      \begin{minipage}{0.4\textwidth}
        \centering
        $$
        \begin{matrix}
          JP & : & 0 & 1 & 2 & 3 & \cdots & N-1 \\
           J & : & 0 & 2 & 4 & 5 &        &     \\
          JS & : & 0 & 0 & 0 & 0 & \cdots &  0  \\
           V & : & 1 & 2 & 3 & 4 &        &     \\
          VS & : & 0 & 0 & 0 & 0 & \cdots &  0  \\
          \RS & : & 4 & 4 & 4 & 4 & \cdots &  4  \\
        \end{matrix}
        $$
      \end{minipage}
      \caption[Best-case sparse matrix with corresponding C3SR representation.]{\textbf{Best-case sparse matrix with corresponding C3SR representation.} The matrix exhibits a single pattern and all of its rows have the same values leading to minimally sized J and V optimal for the repeated accesses required for matrix-vector multiplication.}
      \label{fig:c3sr-example-best-case}
    \end{figure}

    Note that the structure of a sparse banded matrix derived from a fully structured grid is exclusively diagonal or
    locally uniform, i.e. the non-zeros either align into multiple diagonals as in Figure \ref{fig:laplacian-example} or
    contain blocks of square matrices when solving for multiple coupled entities. Thus the concept of decomposing a
    row's non-zeros' column indices into its peg index and patterns is slightly redundant but is retained for the sake
    of flexibility when dealing with matrices associated with grids which contain structured segments. \Todo{Absatz
    rausschmeißen?}

    \Todo{Prämisse der korrekten Indizierung der Gitterknoten explizit betonen.}

  \section{Matrix-Vector Multiplication Schemes} \label{subsec:matrix-vector-multiplication-schemes}

    Aside from the basic CSR-like matrix-vector multiplication scheme, the C3SR format's storage scheme allows for a
    vectorized arithmetic scheme for sparse banded matrices. This section details the different matrix-vector
    multiplication schemes whose performance is then gauged in the subsequent section.

    \Todo{Nomenklatur 'scalar' für CSR-Schema einführen und verwenden}

    \subsection{Basic CSR-like Multiplication Scheme} \label{subsubsec:basic-csr-like-multiplication-scheme}

      Algorithmically, the basic row-by-column matrix-vector multiplication scheme of the C3SR format is similar to that
      of the CSR format. Differences arise only in the C3SR's additional offset $JP[k]$ which is applied to each
      relative column index prior to accessing the argument vector $x$ as shown in Figure
      \ref{fig:csr-vs-c3sr-mvm-scalar}.

      \begin{figure}[ht]
        \centering
        \captionsetup{width=0.9\columnwidth}
        \input{./assets/csr-vs-c3sr-mvm-scalar.tex}
        \toccaption{Matrix-vector multiplication schemes of the C3SR and CSR format in comparison.}
        {The $k^{\text{th}}$ component of the product $Ax$ is shown. The C3SR format accesses V and J from their
        dedicated index-pointer arrays while the CSR format utilizes one common row-pointer array \RP. Additionally, the
        C3SR format requires the relative column indices to be offset by the peg column index for each row.}
        \label{fig:csr-vs-c3sr-mvm-scalar}
      \end{figure}

      As the C3SR format utilizes additional arrays to store index-pointers the number of memory accesses increases
      likewise. Assuming a general case of a large sparse matrix devoid of any regularity in its structure whose size
      exceeds the machine's cache capacity the CSR format's arithmetic performance will be better proportional to the
      difference in the storage size as the above schemes yield memory bound computations involving few trivial
      arithmetic operations.

      On the flipside, as demonstrated in the previous section sparse banded matrices facilitate very compact storage
      allowing a small segment of memory, corresponding to possibly only a few cache lines, to contain the matrix's
      complete structural information and, depending on the underlying physical problem, even the numeric values. This
      can drastically improve the arithmetic performance of the C3SR format despite the more complex memory access
      scheme as will be shown in the benchmark section.

    \subsection{Vectorized SIMD Multiplication Scheme} \label{subsubsec:vectorized-simd-multiplication-scheme}

      Aside of the performance benefits due to smaller object sizes and better data locality sparse banded matrices
      derived from structured grids stored in C3SR-format exhibit data layouts particularly well suited for a vectorized
      implementation of matrix-vector multiplication.

      Seeing as the standard CSR-format's implementation of sparse matrix-vector multiplication is a memory-bound
      operation for large matrices which necessitates loading data from main memory, the drastic decrease in object size
      when applying the C3SR-format's compression scheme can reduce the object size enough for it to fit into cache
      memory whose higher bandwidth may allow to increase arithmetic performance through vectorization. Such
      benefits are verified in the benchmark section\Todo{Add reference to mvm}.

      Depending on the source matrix's structure and values its data is laid out in memory such that matrix-vector
      multiplication can be performed simultaneously for multiple rows using vectorization. Several configurations of
      the values and the structure of a matrix slice, for which the matrix-vector multiplication can be vectorized,
      exist and arise in real-world applications. The configurations require similar data access patterns and
      implementations which shall be presented here alongside their corresponding matrix slice. The different
      implementations will be referred to as \keyword{SIMD schemes}.

      \subsubsection{Matrix Slice Compositions and corresponding SIMD Schemes}

        For the implementation of vectorized matrix-vector multiplication for a matrix slice its non-zeros' values are
        assigned one of two categories: If all rows share the same values in the same order of the appearance of
        non-zeros in the row, the values' composition is referred to as \keyword{collated}, named after the fact that by
        virtue of the C3SR-format's storage scheme said set of values is stored only once in memory, hence collating the
        values for all rows in the matrix slice into a single segmment. Note that the non-zeros' column indices are of
        no concern here and may differ between rows as long as the values are identical. Conversely, if the matrix
        slice's values are not collated it's composition is referred to as \keyword{arbitrary}.

        Similarly to a matrix slice's values its structure falls into one of three categories: If the non-zeros are
        aligned in dense diagonals its structure is referred to as \keyword{diagonal}. A \keyword{uniform} structure is
        present if all non-zeros are aligned into vertical columns, i.e. every row's non-zeros share the same column
        indices. Finally, if the matrix slice's structural composition is neither diagonal nor uniform, it is assigned
        the default of \keyword{arbitrary}.

        Thus a total of $2 \times 3 = 6$ different data access patterns and implementations are considered, arising from
        all possible combinations of the values' and the structural compositions. For the sake of brevity a tuple of a
        matrix slice's values' composition and its structural composition shall be referred to as the \keyword{slice's
        composition}. Each variant of slice composition is detailed in the following paragraphs along with its data
        access patterns required to vectorize matrix-vector multiplication.

        Note that the concepts introduced herein are generic and apply to matrix slices with arbitrary numbers of rows.
        A specific choice of the number of rows in a slice must depend on the underlying hardware's vector register size
        and is further detailed in the corresponding implementation section (see \Todo{Ref impl:mvm-vectorized}). This
        section serves to introduce the basic idea.

        \Todo{Architekturen mit schneller Stride-Gather Instruktion suchen}

        \Todo{Slice Composition hängt von genauer Datenpositionierung ab und kann nicht ausschließlich aus dem Slice
        bestimmt werden. Frühere Reihen könnten selbe Values gehabt haben, was aber Sonderfall ist. Slicecompo wird aus
        tatsächlichem Datenlayout bestimmt und nicht aus der Teilmatrix, daher nono}

        \paragraph{SIMD Scheme I: Arbitrary Values \& Diagonal Structure}

          Suppose that a horizontal slice of rows $r, \ldots, r+k$ has a diagonal structure with the diagonals starting
          at indices $s, t, \ldots $, such as is depicted in the upper segment of Figure
          \ref{fig:simd-scheme-i-data-layout}. The matrix-vector multiplication for each of the slice's rows is composed
          of the same number of summands, one per diagonal. Each of the summands is a product of the diagonal's non-zero
          entry and the argument's corresponding element, starting out with $a_{r,s} \cdot x_s$ for the first diagonal's
          initial element and $a_{r,t} \cdot x_t$ for the second diagonal. For each subsequent row the matrix element's
          indices are incremented as well as the argument vector's index, commencing with $a_{r+1, s+1} \cdot x_{s+1}$
          for the first diagonal's second element and accordingly for each other diagonal.

          \begin{figure}[H]
            \centering
            \captionsetup{width=0.9\columnwidth}
            \input{./assets/simd-scheme-i-data-layout.tex}
            \toccaption{Matrix slice with arbitrary values and diagonal structure, corresponding data layout and
              arithmetic scheme.}{Section I shows a matrix slice with arbitrary values and diagonal structure. Row and
              column indices are indicated at the matrix's perimeter. Section II displays the slice's corresponding data
              layout. Pointer-index values are indicated by arrows to the elements they point. Only arrays relevant to
              the arithmetic scheme are sketched. Section III shows the arithmetic scheme for a slice of $k$ rows which
              is translated into operations on vector registers.}
            \label{fig:simd-scheme-i-data-layout}
          \end{figure}

          As each diagonal's summand accesses the argument vector's elements in consecutive fashion, for example, $x_s,
          x_{s+1}, \ldots, x_{s + k}$ for the first diagonal, the access to the argument vector's elements can be
          vectorized. The other summand's constituents, the diagonal's elements, are stored within V at a fixed stride
          which is equal to the number of diagonals. This is due to the fact that the matrix slice's non-zero elements
          are stored row-by-row while each row has the same number of elements. Thus, for the general case when the
          rows' values don't match, $a_{r, s}$ is located as many elements before $a_{r+1, s+1}$ within V as there are
          diagonals.

          In SIMD terms a stride-gather and a load are required for the diagonal's elements and the argument vector,
          respectively. The two vector registers are then multiplied and the result is then added onto the next product
          pertaining to the subsequent diagonal. After the final vectorized addition the result is written into the
          output buffer using a vectorized store. Figure \ref{fig:simd-scheme-i-data-layout} shows the matrix's data
          layout in memory and the arithmetic scheme.

        \paragraph{SIMD Scheme II: Collated Values \& Diagonal Structure}

          The second SIMD scheme applies to matrix slice with collated values and diagonal structure. As all values
          within a diagonal are identical, e.g. $a_{r,s} = a_{r+1, s+1} = \ldots$ for the first diagonal, the arithmetic
          is simplified with respect to the previous scheme. Instead of being subjected to a vectorized multiplication,
          the argument vector's values are simply scaled by the diagonal's value. Figure \ref{fig:simd-scheme-ii-data-layout})
          shows the data layout and arithmetic scheme resulting from this configuration.

          \begin{figure}[H]
            \centering
            \captionsetup{width=0.9\columnwidth}
            \input{./assets/simd-scheme-ii-data-layout.tex}
            \toccaption{Matrix slice with collated values and diagonal structure, corresponding data layout and
              arithmetic scheme.}{Section I shows a matrix slice with collated values and diagonal structure. Row and
              column indices are indicated at the matrix's perimeter. Section II displays the slice's corresponding data
              layout. Pointer-index values are indicated by arrows to the elements they point. Only arrays relevant to
              the arithmetic scheme are sketched. Section III shows the arithmetic scheme for a slice of $k$ rows which
              is translated into operations on vector registers.}
            \label{fig:simd-scheme-ii-data-layout}
          \end{figure}

        \paragraph{SIMD Scheme III: Arbitrary Values \& Uniform Structure}

          The third SIMD scheme may arise when solving PDEs on coupled entities producing matrix slices of uniform
          structure and arbitrary values. This leads to the circumstance that the argument vector's element required for
          the partial sum involving a matrix column is constant for the whole column as outlined in Figure
          \ref{fig:simd-scheme-iii-data-layout}. Thus the argument vector's elements serve as scaling constants for the
          vector register containing the matrix's elements which, as in the first configuration, are spread at a fixed
          stride throughout V.

          \begin{figure}[H]
            \centering
            \captionsetup{width=0.9\columnwidth}
            \input{./assets/simd-scheme-iii-data-layout.tex}
            \toccaption{Matrix slice with arbitrary values and uniform structure, corresponding data layout and
              arithmetic scheme.}{Section I shows a matrix slice with arbitrary values and uniform structure. Row and
              column indices are indicated at the matrix's perimeter. Section II displays the slice's corresponding data
              layout. Pointer-index values are indicated by arrows to the elements they point. Only arrays relevant to
              the arithmetic scheme are sketched. Section III shows the arithmetic for a slice of $k$ rows scheme which
              is translated into operations on vector registers.}
            \label{fig:simd-scheme-iii-data-layout}
          \end{figure}

        \paragraph{SIMD Scheme IV: Collated Value \& Uniform Structure}

          Matrix slices of uniform structure and collated values exhibit multiple non-zero columns while each column is
          made from the same numeric value. This comprises the most trivial SIMD scheme as the conjunction of the
          argument vector's values and the matrix's values collapses to a single scalar multiplication.

          \begin{figure}[H]
            \centering
            \captionsetup{width=0.9\columnwidth}
            \input{./assets/simd-scheme-iv-data-layout.tex}
            \toccaption{Matrix slice with collated values and uniform structure, corresponding data layout and
              arithmetic scheme.}{Section I shows a matrix slice with collated values and uniform structure. Row and
              column indices are indicated at the matrix's perimeter. Section II displays the slice's corresponding data
              layout. Pointer-index values are indicated by arrows to the elements they point. Only arrays relevant to
              the arithmetic scheme are sketched. Section III shows the arithmetic scheme for a slice of $k$ rows which
              is translated into operations on vector registers.}
            \label{fig:simd-scheme-iv-data-layout}
          \end{figure}

        \paragraph{SIMD Scheme V: Fallback Case for Arbitrary Structure Composition}

          If the matrix slice's structure is neither diagonal nor uniform a fallback scheme is applied regardless of how
          the slice's numerical values are composed. Contrasting the previous configurations, an arbitrary structural
          composition requires memory accesses which cannot be assumed to adhere to data locality or to follow any
          simple pattern. Thus the conditions for which vectorization is expected to provide performance benefits do not
          hold and the scalar multiplication scheme described in \ref{fig:csr-vs-c3sr-mvm-scalar} is utilized for the
          matrix slice.
